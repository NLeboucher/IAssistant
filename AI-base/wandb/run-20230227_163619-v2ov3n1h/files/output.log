
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.1005, 'learning_rate': 5.314366948180494e-08, 'epoch': 0.01}
{'loss': 1.1006, 'learning_rate': 1.0628733896360988e-07, 'epoch': 0.01}
{'loss': 1.0999, 'learning_rate': 1.594310084454148e-07, 'epoch': 0.02}
{'loss': 1.0992, 'learning_rate': 2.1257467792721976e-07, 'epoch': 0.02}
{'loss': 1.0981, 'learning_rate': 2.657183474090247e-07, 'epoch': 0.03}
{'loss': 1.0981, 'learning_rate': 3.188620168908296e-07, 'epoch': 0.03}
{'loss': 1.096, 'learning_rate': 3.7200568637263454e-07, 'epoch': 0.04}
{'loss': 1.0969, 'learning_rate': 4.251493558544395e-07, 'epoch': 0.04}
{'loss': 1.0952, 'learning_rate': 4.782930253362444e-07, 'epoch': 0.05}
{'loss': 1.0899, 'learning_rate': 5.314366948180494e-07, 'epoch': 0.05}
{'loss': 1.0828, 'learning_rate': 5.845803642998543e-07, 'epoch': 0.06}
{'loss': 1.0602, 'learning_rate': 6.377240337816592e-07, 'epoch': 0.06}
