
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.0985, 'learning_rate': 3.98577521113537e-08, 'epoch': 0.01}
{'loss': 1.1004, 'learning_rate': 7.97155042227074e-08, 'epoch': 0.01}
{'loss': 1.0996, 'learning_rate': 1.195732563340611e-07, 'epoch': 0.02}
{'loss': 1.0978, 'learning_rate': 1.594310084454148e-07, 'epoch': 0.02}
{'loss': 1.0987, 'learning_rate': 1.9928876055676853e-07, 'epoch': 0.03}
{'loss': 1.0989, 'learning_rate': 2.391465126681222e-07, 'epoch': 0.03}
{'loss': 1.0957, 'learning_rate': 2.7900426477947593e-07, 'epoch': 0.04}
{'loss': 1.0969, 'learning_rate': 3.188620168908296e-07, 'epoch': 0.04}
{'loss': 1.0954, 'learning_rate': 3.587197690021833e-07, 'epoch': 0.05}
{'loss': 1.0936, 'learning_rate': 3.9857752111353706e-07, 'epoch': 0.05}
{'loss': 1.0942, 'learning_rate': 4.3843527322489075e-07, 'epoch': 0.06}
{'loss': 1.0895, 'learning_rate': 4.782930253362444e-07, 'epoch': 0.06}
{'loss': 1.0807, 'learning_rate': 5.181507774475982e-07, 'epoch': 0.07}
{'loss': 1.0628, 'learning_rate': 5.580085295589519e-07, 'epoch': 0.07}
{'loss': 1.0418, 'learning_rate': 5.978662816703056e-07, 'epoch': 0.08}
{'loss': 1.0231, 'learning_rate': 6.377240337816592e-07, 'epoch': 0.08}
{'loss': 0.9862, 'learning_rate': 6.77581785893013e-07, 'epoch': 0.09}
{'loss': 0.9574, 'learning_rate': 7.174395380043666e-07, 'epoch': 0.09}
{'loss': 0.9167, 'learning_rate': 7.572972901157203e-07, 'epoch': 0.1}
{'loss': 0.876, 'learning_rate': 7.971550422270741e-07, 'epoch': 0.1}
{'loss': 0.8416, 'learning_rate': 8.370127943384277e-07, 'epoch': 0.11}
{'loss': 0.8096, 'learning_rate': 8.768705464497815e-07, 'epoch': 0.11}
{'loss': 0.7768, 'learning_rate': 9.167282985611352e-07, 'epoch': 0.12}
{'loss': 0.7438, 'learning_rate': 9.565860506724888e-07, 'epoch': 0.12}
{'loss': 0.7224, 'learning_rate': 9.964438027838426e-07, 'epoch': 0.13}
{'loss': 0.6981, 'learning_rate': 1.0363015548951964e-06, 'epoch': 0.13}
{'loss': 0.6693, 'learning_rate': 1.07615930700655e-06, 'epoch': 0.14}
{'loss': 0.6905, 'learning_rate': 1.1160170591179037e-06, 'epoch': 0.14}
{'loss': 0.6545, 'learning_rate': 1.1558748112292573e-06, 'epoch': 0.15}
{'loss': 0.6329, 'learning_rate': 1.1957325633406111e-06, 'epoch': 0.15}
{'loss': 0.6073, 'learning_rate': 1.235590315451965e-06, 'epoch': 0.16}
{'loss': 0.6569, 'learning_rate': 1.2754480675633185e-06, 'epoch': 0.16}
{'loss': 0.6708, 'learning_rate': 1.315305819674672e-06, 'epoch': 0.17}
{'loss': 0.6727, 'learning_rate': 1.355163571786026e-06, 'epoch': 0.17}
{'loss': 0.6631, 'learning_rate': 1.3950213238973797e-06, 'epoch': 0.18}
{'loss': 0.6511, 'learning_rate': 1.4348790760087333e-06, 'epoch': 0.18}
{'loss': 0.6355, 'learning_rate': 1.474736828120087e-06, 'epoch': 0.19}
{'loss': 0.6653, 'learning_rate': 1.5145945802314406e-06, 'epoch': 0.19}
{'loss': 0.6487, 'learning_rate': 1.5544523323427944e-06, 'epoch': 0.2}
{'loss': 0.6073, 'learning_rate': 1.5943100844541482e-06, 'epoch': 0.2}
{'loss': 0.6604, 'learning_rate': 1.6341678365655018e-06, 'epoch': 0.21}
{'loss': 0.6537, 'learning_rate': 1.6740255886768554e-06, 'epoch': 0.21}
{'loss': 0.6706, 'learning_rate': 1.7138833407882094e-06, 'epoch': 0.22}
{'loss': 0.6323, 'learning_rate': 1.753741092899563e-06, 'epoch': 0.22}
{'loss': 0.623, 'learning_rate': 1.7935988450109166e-06, 'epoch': 0.23}
The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx. If premise, hypothesis, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9815
  Batch size = 4
{'eval_loss': 0.6339808106422424, 'eval_accuracy': 0.813143148242486, 'eval_runtime': 127.4385, 'eval_samples_per_second': 77.018, 'eval_steps_per_second': 19.256, 'epoch': 0.23}
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-22581
Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-22581/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-22581/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-22581/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-22581/special_tokens_map.json
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Several commits (3) will be pushed upstream.
{'train_runtime': 5608.9336, 'train_samples_per_second': 16.103, 'train_steps_per_second': 4.026, 'train_loss': 0.84531744459502, 'epoch': 0.23}
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-22581 (score: 0.813143148242486).
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli
