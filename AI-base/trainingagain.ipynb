{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW,AutoTokenizer, AutoModelForSequenceClassification,get_linear_schedule_with_warmup, TrainingArguments, Trainer\n",
    "from torch.optim import Adadelta, Adam, SGD, RMSprop, AdamW\n",
    "import wandb\n",
    "#from transformers.trainer_utils import A\n",
    "#%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Arguments\n",
    "model_checkpoint = \"microsoft/mpnet-base\"#\"symanto/mpnet-base-snli-mnli\" got 87% on MNLI\n",
    "MODEL_NAME = model_checkpoint.split(\"/\")[-1]\n",
    "MODEL = AutoModelForSequenceClassification.from_pretrained\n",
    "METRIC_NAME = \"accuracy\"\n",
    "BATCHSIZE = 4\n",
    "OPTIMIZER=AdamW #AdamW, SGD, Adadelta\n",
    "EPOCHNUM = 0.20\n",
    "WEIGHT_DECAY = 0.8\n",
    "TOKENIZER=AutoTokenizer.from_pretrained\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "metric = load_metric('glue', \"mnli\")\n",
    "num_labels = 3 # for mnli\n",
    "optimizer_name = \"AdamW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 7596, 1014, 2027, 2032, 6255, 1003, 2, 2, 2002, 2027, 6255, 3636, 2011, 2013, 1016, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = TOKENIZER(model_checkpoint, use_fast=True) # for mnli\n",
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\", truncation=True) # for mnli\n",
    "MODEL=MODEL(model_checkpoint, num_labels=num_labels) # for mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Sentence 2: Product and geography are what make cream skimming work. \n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = (\"premise\", \"hypothesis\") # for mnli\n",
    "\n",
    "print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[sentence1_key],\n",
    "        examples[sentence2_key],\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e918085800bdaf35.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19eaff48a7946f26.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fbc8360796e1193d.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d767475232a1637b.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-61b7eb49a8b6e726.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{MODEL_NAME}-finetuned-{optimizer_name}-mnli\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=6.7e-5,#6.7e-5(81.7),#1.8e-5,#2.4e-5 , #3e-5 AdamW \n",
    "    per_device_train_batch_size=BATCHSIZE,\n",
    "    per_device_eval_batch_size=BATCHSIZE,\n",
    "    num_train_epochs=EPOCHNUM,                                          \n",
    "    weight_decay=WEIGHT_DECAY,                                               \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC_NAME,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "#accuracy score 82.4    83.1   81.7   82.17    81.7\n",
    "#lr             6.7e-5  6.7e-5 6.7e-5 6.7e-5   6.7e-5   6.7e-4     6.7e-4  6.7e-4\n",
    "#weight_decay   1.14  1.2ou1.1 1.0    0.8      0.8      0.01       0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0 392702 0.2\n"
     ]
    }
   ],
   "source": [
    "# #estimated_training_size =args.size_of_dataset if args.size_of_dataset is not None else len(encoded_dataset[\"train\"])\n",
    "# estimated_training_size = len(encoded_dataset[\"train\"])\n",
    "# num_epoch = args.num_train_epochs*100 if args.num_train_epochs is not None else 1\n",
    "\n",
    "# print(num_epoch, estimated_training_size,args.num_train_epochs)\n",
    "# total_step = estimated_training_size * num_epoch // args.per_device_train_batch_size\n",
    "# if total_step <= 0:\n",
    "#         t_total = estimated_training_size * num_epoch // args.actual_train_batch_size\n",
    "# else:\n",
    "#     t_total = total_step\n",
    "\n",
    "# if args.warmup_steps <= 0:  # set the warmup steps to 0.1 * total step if the given warmup step is -1.\n",
    "#     args.warmup_steps = int(t_total * 0.1)\n",
    "# no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": args.weight_decay,\n",
    "#     },\n",
    "#     {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "# ]\n",
    "\n",
    "# optimizer = OPTIMIZER(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "# #optimizer = SGD(optimizer_grouped_parameters, lr=args.learning_rate, momentum=0.0, weight_decay=args.weight_decay) #does not get satisfying results lr=3e-6 3e-5 3e-4 3e-3 3e-2\n",
    "# #optimizer=Adadelta(optimizer_grouped_parameters, lr=args.learning_rate, weight_decay=args.weight_decay,) #lr=3e-6 #does not get satisfying results either\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/Documents/IAS2/IAssistant/AI-base/mpnet-base-finetuned-AdamW-mnli is already a clone of https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    MODEL,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation_matched\"],#for mnli\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #optimizers=(optimizer, scheduler),\n",
    "    #optimizers=(transformers.AdamW, transformers.get_scheduler(\"linear\", num_warmup_steps=0, num_training_steps=args.num_train_epochs*len(encoded_dataset[\"train\"])/args.per_device_train_batch_size)),\n",
    "    #token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx. If hypothesis, premise, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/nico/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19636\n",
      "  Number of trainable parameters = 109488771\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-leboucher\u001b[0m (\u001b[33miassistant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nico/Documents/IAS2/IAssistant/AI-base/wandb/run-20230228_203236-j2utmsfv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iassistant/huggingface/runs/j2utmsfv' target=\"_blank\">amber-grass-17</a></strong> to <a href='https://wandb.ai/iassistant/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iassistant/huggingface' target=\"_blank\">https://wandb.ai/iassistant/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iassistant/huggingface/runs/j2utmsfv' target=\"_blank\">https://wandb.ai/iassistant/huggingface/runs/j2utmsfv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb8e14b50ea40479c412abb9242b6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0972, 'learning_rate': 1.7061283110348304e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0993, 'learning_rate': 3.412256622069661e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0991, 'learning_rate': 5.118384933104492e-07, 'epoch': 0.02}\n",
      "{'loss': 1.0964, 'learning_rate': 6.824513244139321e-07, 'epoch': 0.02}\n",
      "{'loss': 1.0964, 'learning_rate': 8.530641555174153e-07, 'epoch': 0.03}\n",
      "{'loss': 1.0885, 'learning_rate': 1.0236769866208984e-06, 'epoch': 0.03}\n",
      "{'loss': 1.0412, 'learning_rate': 1.1942898177243815e-06, 'epoch': 0.04}\n",
      "{'loss': 0.9571, 'learning_rate': 1.3649026488278643e-06, 'epoch': 0.04}\n",
      "{'loss': 0.8847, 'learning_rate': 1.5355154799313475e-06, 'epoch': 0.05}\n",
      "{'loss': 0.82, 'learning_rate': 1.7061283110348306e-06, 'epoch': 0.05}\n",
      "{'loss': 0.7683, 'learning_rate': 1.8767411421383134e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7126, 'learning_rate': 2.047353973241797e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7062, 'learning_rate': 2.2179668043452795e-06, 'epoch': 0.07}\n",
      "{'loss': 0.664, 'learning_rate': 2.388579635448763e-06, 'epoch': 0.07}\n",
      "{'loss': 0.647, 'learning_rate': 2.559192466552246e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6543, 'learning_rate': 2.7298052976557286e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6335, 'learning_rate': 2.900418128759212e-06, 'epoch': 0.09}\n",
      "{'loss': 0.701, 'learning_rate': 3.071030959862695e-06, 'epoch': 0.09}\n",
      "{'loss': 0.6602, 'learning_rate': 3.2416437909661777e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6874, 'learning_rate': 3.412256622069661e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6518, 'learning_rate': 3.582869453173144e-06, 'epoch': 0.11}\n",
      "{'loss': 0.6871, 'learning_rate': 3.753482284276627e-06, 'epoch': 0.11}\n",
      "{'loss': 0.6864, 'learning_rate': 3.92409511538011e-06, 'epoch': 0.12}\n",
      "{'loss': 0.6193, 'learning_rate': 4.094707946483594e-06, 'epoch': 0.12}\n",
      "{'loss': 0.6644, 'learning_rate': 4.265320777587076e-06, 'epoch': 0.13}\n",
      "{'loss': 0.6685, 'learning_rate': 4.435933608690559e-06, 'epoch': 0.13}\n",
      "{'loss': 0.636, 'learning_rate': 4.606546439794043e-06, 'epoch': 0.14}\n",
      "{'loss': 0.678, 'learning_rate': 4.777159270897526e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6722, 'learning_rate': 4.947772102001009e-06, 'epoch': 0.15}\n",
      "{'loss': 0.5993, 'learning_rate': 5.118384933104492e-06, 'epoch': 0.15}\n",
      "{'loss': 0.6024, 'learning_rate': 5.288997764207974e-06, 'epoch': 0.16}\n",
      "{'loss': 0.627, 'learning_rate': 5.459610595311457e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6319, 'learning_rate': 5.630223426414941e-06, 'epoch': 0.17}\n",
      "{'loss': 0.6517, 'learning_rate': 5.800836257518424e-06, 'epoch': 0.17}\n",
      "{'loss': 0.6374, 'learning_rate': 5.971449088621907e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6172, 'learning_rate': 6.14206191972539e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6305, 'learning_rate': 6.312674750828872e-06, 'epoch': 0.19}\n",
      "{'loss': 0.6381, 'learning_rate': 6.483287581932355e-06, 'epoch': 0.19}\n",
      "{'loss': 0.6338, 'learning_rate': 6.653900413035839e-06, 'epoch': 0.2}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli\n",
      "Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json\n",
      "Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin\n",
      "tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json\n",
      "Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab6ccf0e05d4061b51a89b7d837feed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli\n",
      "   a193af4..46ef91a  main -> main\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(model_name=f\"{MODEL_NAME}-finetuned-{optimizer_name}-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fitetuningpretrained pytorch from scratch\n",
    "# del model\n",
    "# del trainer\n",
    "# del args\n",
    "# #del batch\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
