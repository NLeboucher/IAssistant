{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BiLSTM\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# ensure that the same sequence of random numbers will be generated every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<torchtext.data.field.LabelField object at 0x7f32f8c44b20>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEXT = data.Field(tokenize=tokenizer, lower = True)\n",
    "LABEL = data.LabelField()\n",
    "print(TEXT.dtype)\n",
    "print(LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)\n",
    "#loading data from the SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ,\n",
    "                 vectors = \"glove.6B.300d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "#building the vocabulary for the text and label fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnique tokens in TEXT vocabulary: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(TEXT\u001b[39m.\u001b[39mvocab)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnique tokens in LABEL vocabulary: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(LABEL\u001b[39m.\u001b[39mvocab)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDebug: These two lines should show be the same words (for TEXT) \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mTEXT\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mfreqs\u001b[39m.\u001b[39mmost_common(\u001b[39m20\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mTEXT\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mitos[:\u001b[39m20\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m#showing the most common words in the vocabulary good if they are the same as the above frequency list\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDebug: These two lines should show be the same words (for LABEL)\u001b[39m\u001b[39m{\u001b[39;00mLABEL\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mfreqs\u001b[39m.\u001b[39mmost_common()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m#showing the frequency of the labels in the vocabulary\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mLABEL\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mitos\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\"\n",
    "        f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\"\n",
    "        f\"Debug: These two lines should show be the same words (for TEXT) \\n{TEXT.vocab.freqs.most_common(20)}\"\n",
    "        f\"{TEXT.vocab.itos[:20]}\" #showing the most common words in the vocabulary good if they are the same as the above frequency list\n",
    "\n",
    "        f\"Debug: These two lines should show be the same words (for LABEL){LABEL.vocab.freqs.most_common()}\" #showing the frequency of the labels in the vocabulary\n",
    "        f\"{LABEL.vocab.itos}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "# creating iterators for the data \n",
    "print(f\"{device}, {'gpu' if device == 'cuda' else 'slow training on cpu'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "N_LSTM_LAYERS = 2\n",
    "N_FC_LAYERS = 3\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = BiLSTM.NLIBiLSTM(INPUT_DIM,\n",
    "                  EMBEDDING_DIM,\n",
    "                  HIDDEN_DIM,\n",
    "                  N_LSTM_LAYERS,\n",
    "                  N_FC_LAYERS,\n",
    "                  OUTPUT_DIM,\n",
    "                  DROPOUT,\n",
    "                  PAD_IDX).to(device)\n",
    "# creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "print(model.sumary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.cuda.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        prem = batch.premise\n",
    "        hypo = batch.hypothesis\n",
    "        labels = batch.label\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #prem = [prem sent len, batch size]\n",
    "        #hypo = [hypo sent len, batch size]\n",
    "        \n",
    "        predictions = model(prem, hypo)\n",
    "        \n",
    "        #predictions = [batch size, output dim]\n",
    "        #labels = [batch size]\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "                \n",
    "        acc = categorical_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            prem = batch.premise\n",
    "            hypo = batch.hypothesis\n",
    "            labels = batch.label\n",
    "                        \n",
    "            predictions = model(prem, hypo)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "                \n",
    "            acc = categorical_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_inference(premise, hypothesis, text_field, label_field, model, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(premise, str):\n",
    "        premise = text_field.tokenize(premise)\n",
    "    \n",
    "    if isinstance(hypothesis, str):\n",
    "        hypothesis = text_field.tokenize(hypothesis)\n",
    "    \n",
    "    if text_field.lower:\n",
    "        premise = [t.lower() for t in premise]\n",
    "        hypothesis = [t.lower() for t in hypothesis]\n",
    "        \n",
    "    premise = [text_field.vocab.stoi[t] for t in premise]\n",
    "    hypothesis = [text_field.vocab.stoi[t] for t in hypothesis]\n",
    "    \n",
    "    premise = torch.LongTensor(premise).unsqueeze(1).to(device)\n",
    "    hypothesis = torch.LongTensor(hypothesis).unsqueeze(1).to(device)\n",
    "    \n",
    "    prediction = model(premise, hypothesis)\n",
    "    \n",
    "    prediction = prediction.argmax(dim=-1).item()\n",
    "    \n",
    "    return label_field.vocab.itos[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m hypothesis \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma woman sitting on a green bench.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m750\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     predict_inference(premise, hypothesis, TEXT, LABEL, model, device)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_inference' is not defined"
     ]
    }
   ],
   "source": [
    "premise = 'a man sitting on a green bench.'\n",
    "hypothesis = 'a woman sitting on a green bench.'\n",
    "for i in range(750):\n",
    "    predict_inference(premise, hypothesis, TEXT, LABEL, model, device)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = 'Move the sun to the right and above.'\n",
    "hypothesis = 'A user wants to increase the sound.'\n",
    "\n",
    "predict_inference(premise, hypothesis, TEXT, LABEL, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = 'Move the sun to the right'\n",
    "hypothesis = 'A user wants the sun to move to the top.'\n",
    "\n",
    "predict_inference(premise, hypothesis, TEXT, LABEL, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = 'a man sitting on a green bench.'\n",
    "hypothesis = 'a person on a park bench'\n",
    "\n",
    "predict_inference(premise, hypothesis, TEXT, LABEL, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
