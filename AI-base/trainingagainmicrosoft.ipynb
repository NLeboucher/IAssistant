{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW,AutoTokenizer, AutoModelForSequenceClassification,get_linear_schedule_with_warmup, TrainingArguments, Trainer\n",
    "from torch.optim import Adadelta, Adam, SGD, RMSprop, AdamW\n",
    "import wandb\n",
    "#from transformers.trainer_utils import A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mnli\"\n",
    "model_checkpoint = \"microsoft/mpnet-base\"#\"symanto/mpnet-base-snli-mnli\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c2676934c94a9fb5e7c8b8830813c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5414/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For instance, the Post also has the story about the woman meeting with Clinton just days before his first Inaugural, but adds the detail that she says all the encounters were innocent.</td>\n",
       "      <td>A woman met with Clinton days before his first Inaugural but it wasn't thought to be dirty.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>132307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sky had ripped again, and this time the entire dome shook with the shock.</td>\n",
       "      <td>The dome was shaking as the sky ripped once more.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>360074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it's really scary that you have to be that scared about your kids but i mean you do better safe than sorry</td>\n",
       "      <td>It's bad that you have to be that scared for your kids.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>293276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He had no time to ask.</td>\n",
       "      <td>There was no time to question it.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>70657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Further Reform Is Needed to Address Long-Standing Problems</td>\n",
       "      <td>There is a need for more reform.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It told me that Mrs. Inglethorp had been writing the word 'possessed' that afternoon, and, having the fragment of paper found in the grate fresh in my mind, the possibility of a will, (a document almost certain to contain that word), occurred to me at once.</td>\n",
       "      <td>I thought the will may contain the word possessed.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>263292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fortunately, many of these lawyers are willing and able to help in other ways.</td>\n",
       "      <td>Many lawyers are available to help in a variety of ways.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>252147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>But the components of that growth tell the real story.</td>\n",
       "      <td>The real story is hard to come by without the components of that growth.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>352362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The others fled.</td>\n",
       "      <td>The others fled from where they were.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>329686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>By the way, Hastings, there is something I want you to do for me.</td>\n",
       "      <td>I have a favor that I want to ask of you.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>7729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.625}\n"
     ]
    }
   ],
   "source": [
    "#print(metric)\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "print(metric.compute(predictions=fake_preds, references=fake_labels,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 7596, 1014, 2027, 2032, 6255, 1003, 2, 2, 2002, 2027, 6255, 3636, 2011, 2013, 1016, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Sentence 2: Product and geography are what make cream skimming work. \n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     if sentence2_key is None:\n",
    "#         return tokenizer(examples[sentence1_key], truncation=True)\n",
    "#     return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], padding=True, truncation=True)\n",
    "    return tokenizer(\n",
    "        examples[sentence1_key],\n",
    "        examples[sentence2_key],\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 17162, 2139, 6953, 8305, 25061, 2042, 2052, 3941, 9650, 1015, 4035, 2002, 10509, 1016, 2, 2, 4035, 2002, 10509, 2028, 2058, 2195, 6953, 8305, 25061, 2151, 1016, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2021, 2117, 2080, 2000, 2165, 2002, 1049, 3988, 2016, 2016, 2119, 2508, 7914, 2021, 4562, 2072, 2004, 2000, 2283, 2508, 2069, 2069, 2031, 5634, 2004, 9135, 2000, 2000, 6691, 2140, 2000, 13984, 5634, 2004, 2659, 2004, 9135, 1041, 3128, 2017, 6424, 1041, 2063, 1041, 3317, 1041, 3128, 3636, 2043, 2004, 5676, 2036, 2002, 1041, 2313, 1041, 3128, 3636, 2043, 2004, 5676, 2036, 2, 2, 2021, 4562, 2000, 2481, 2004, 2000, 2210, 2508, 2069, 2000, 2115, 9135, 1016, 2], [0, 2032, 2001, 2260, 2197, 2101, 4291, 2045, 2119, 8132, 3375, 2139, 1016, 2, 2, 1041, 2270, 2001, 2030, 2140, 2101, 15393, 2119, 4453, 2011, 14273, 11722, 1016, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2133, 2083, 2021, 2117, 1033, 2039, 2027, 2007, 2041, 2596, 2157, 1016, 2, 2, 2027, 2596, 7464, 2004, 2072, 1016, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 3402, 1049, 2429, 2021, 2058, 2299, 2069, 2021, 2179, 3980, 2074, 2001, 2220, 5097, 6011, 1049, 2068, 2160, 2343, 2089, 2021, 2117, 2031, 1009, 2132, 2897, 2043, 2003, 2000, 3638, 7926, 2850, 2, 2, 2000, 5097, 6011, 2035, 1041, 2850, 2001, 7601, 1016, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e918085800bdaf35.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19eaff48a7946f26.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fbc8360796e1193d.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d767475232a1637b.arrow\n",
      "Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-61b7eb49a8b6e726.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 4\n",
    "optimizer_name = \"AdamW\" #AdamW, SGD, Adadelta\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{optimizer_name}-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=6.7e-5,#6.7e-5(81.7),#1.8e-5,#2.4e-5 , #3e-5 AdamW\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=0.20,\n",
    "    weight_decay=1.09,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "#run num 1      19      18        18        17      16      15      14      13      12      11      10      9       8       7       6       5       4       3       2       1\n",
    "#accuracy score 82.4    82.8      83.55     83.1    82.4    83.1    81.7   82.17    81.7\n",
    "#lr             6.7e-5  6.7e-5    6.7e-5    6.7e-5  6.7e-5  6.7e-5  6.7e-5 6.7e-5   6.7e-5  6.7e-4  6.7e-4  6.7e-4\n",
    "#weight_decay   1.07    1.05      1.08    1.1     1.14      1.1      1.0    0.8     0.8     0.01    0.5     0.5      0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5\n",
    "#                       real      redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0 392702 0.2\n"
     ]
    }
   ],
   "source": [
    "#estimated_training_size =args.size_of_dataset if args.size_of_dataset is not None else len(encoded_dataset[\"train\"])\n",
    "estimated_training_size = len(encoded_dataset[\"train\"])\n",
    "num_epoch = args.num_train_epochs*100 if args.num_train_epochs is not None else 1\n",
    "\n",
    "print(num_epoch, estimated_training_size,args.num_train_epochs)\n",
    "total_step = estimated_training_size * num_epoch // args.per_device_train_batch_size\n",
    "if total_step <= 0:\n",
    "        t_total = estimated_training_size * num_epoch // args.actual_train_batch_size\n",
    "else:\n",
    "    t_total = total_step\n",
    "\n",
    "if args.warmup_steps <= 0:  # set the warmup steps to 0.1 * total step if the given warmup step is -1.\n",
    "    args.warmup_steps = int(t_total * 0.1)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "#optimizer = SGD(optimizer_grouped_parameters, lr=args.learning_rate, momentum=0.0, weight_decay=args.weight_decay) #pas reussi lr=3e-6 3e-5 3e-4 3e-3 3e-2\n",
    "#optimizer=Adadelta(optimizer_grouped_parameters, lr=args.learning_rate, weight_decay=args.weight_decay,) #lr=3e-6 \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/Documents/IAS2/IAssistant/AI-base/mpnet-base-finetuned-AdamW-mnli is already a clone of https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #optimizers=(optimizer, scheduler),\n",
    "    #optimizers=(transformers.AdamW, transformers.get_scheduler(\"linear\", num_warmup_steps=0, num_training_steps=args.num_train_epochs*len(encoded_dataset[\"train\"])/args.per_device_train_batch_size)),\n",
    "    #token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/nico/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19636\n",
      "  Number of trainable parameters = 109488771\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-leboucher\u001b[0m (\u001b[33miassistant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8441d0fe4304ee5a9d9ded6c488eb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668660616666622, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nico/Documents/IAS2/IAssistant/AI-base/wandb/run-20230301_034938-aqaredit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iassistant/huggingface/runs/aqaredit' target=\"_blank\">lyric-star-23</a></strong> to <a href='https://wandb.ai/iassistant/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iassistant/huggingface' target=\"_blank\">https://wandb.ai/iassistant/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iassistant/huggingface/runs/aqaredit' target=\"_blank\">https://wandb.ai/iassistant/huggingface/runs/aqaredit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da273d5f8df47f2aaeba4315c12b47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1, 'learning_rate': 1.7061283110348304e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0974, 'learning_rate': 3.412256622069661e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0978, 'learning_rate': 5.118384933104492e-07, 'epoch': 0.02}\n",
      "{'loss': 1.097, 'learning_rate': 6.824513244139321e-07, 'epoch': 0.02}\n",
      "{'loss': 1.0967, 'learning_rate': 8.530641555174153e-07, 'epoch': 0.03}\n",
      "{'loss': 1.0904, 'learning_rate': 1.0236769866208984e-06, 'epoch': 0.03}\n",
      "{'loss': 1.0635, 'learning_rate': 1.1942898177243815e-06, 'epoch': 0.04}\n",
      "{'loss': 1.0092, 'learning_rate': 1.3649026488278643e-06, 'epoch': 0.04}\n",
      "{'loss': 0.9365, 'learning_rate': 1.5355154799313475e-06, 'epoch': 0.05}\n",
      "{'loss': 0.8622, 'learning_rate': 1.7061283110348306e-06, 'epoch': 0.05}\n",
      "{'loss': 0.7956, 'learning_rate': 1.8767411421383134e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7268, 'learning_rate': 2.047353973241797e-06, 'epoch': 0.06}\n",
      "{'loss': 0.7159, 'learning_rate': 2.2179668043452795e-06, 'epoch': 0.07}\n",
      "{'loss': 0.6757, 'learning_rate': 2.388579635448763e-06, 'epoch': 0.07}\n",
      "{'loss': 0.6485, 'learning_rate': 2.559192466552246e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6761, 'learning_rate': 2.7298052976557286e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6257, 'learning_rate': 2.900418128759212e-06, 'epoch': 0.09}\n",
      "{'loss': 0.6992, 'learning_rate': 3.071030959862695e-06, 'epoch': 0.09}\n",
      "{'loss': 0.6624, 'learning_rate': 3.2416437909661777e-06, 'epoch': 0.1}\n",
      "{'loss': 0.695, 'learning_rate': 3.412256622069661e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6586, 'learning_rate': 3.582869453173144e-06, 'epoch': 0.11}\n",
      "{'loss': 0.6921, 'learning_rate': 3.753482284276627e-06, 'epoch': 0.11}\n",
      "{'loss': 0.6861, 'learning_rate': 3.92409511538011e-06, 'epoch': 0.12}\n",
      "{'loss': 0.6283, 'learning_rate': 4.094707946483594e-06, 'epoch': 0.12}\n",
      "{'loss': 0.6586, 'learning_rate': 4.265320777587076e-06, 'epoch': 0.13}\n",
      "{'loss': 0.6509, 'learning_rate': 4.435933608690559e-06, 'epoch': 0.13}\n",
      "{'loss': 0.6443, 'learning_rate': 4.606546439794043e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6778, 'learning_rate': 4.777159270897526e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6512, 'learning_rate': 4.947772102001009e-06, 'epoch': 0.15}\n",
      "{'loss': 0.6099, 'learning_rate': 5.118384933104492e-06, 'epoch': 0.15}\n",
      "{'loss': 0.6027, 'learning_rate': 5.288997764207974e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6325, 'learning_rate': 5.459610595311457e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6592, 'learning_rate': 5.630223426414941e-06, 'epoch': 0.17}\n",
      "{'loss': 0.6632, 'learning_rate': 5.800836257518424e-06, 'epoch': 0.17}\n",
      "{'loss': 0.6218, 'learning_rate': 5.971449088621907e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6214, 'learning_rate': 6.14206191972539e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6359, 'learning_rate': 6.312674750828872e-06, 'epoch': 0.19}\n",
      "{'loss': 0.6347, 'learning_rate': 6.483287581932355e-06, 'epoch': 0.19}\n",
      "{'loss': 0.6323, 'learning_rate': 6.653900413035839e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2679b6204747e383825670bedb564a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-19636\n",
      "Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5741181969642639, 'eval_accuracy': 0.8290371879775853, 'eval_runtime': 124.3193, 'eval_samples_per_second': 78.95, 'eval_steps_per_second': 19.739, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/pytorch_model.bin\n",
      "tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/tokenizer_config.json\n",
      "Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/special_tokens_map.json\n",
      "tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json\n",
      "Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-19636 (score: 0.8290371879775853).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4861.8945, 'train_samples_per_second': 16.154, 'train_steps_per_second': 4.039, 'train_loss': 0.7586076246499772, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19636, training_loss=0.7586076246499772, metrics={'train_runtime': 4861.8945, 'train_samples_per_second': 16.154, 'train_steps_per_second': 4.039, 'train_loss': 0.7586076246499772, 'epoch': 0.2})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli\n",
      "Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json\n",
      "Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin\n",
      "tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json\n",
      "Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a52f5950894805b8d8a79983eb08c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli\n",
      "   ffed91e..7b79114  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(metric.compute(predictions=fake_preds, references=fake_labels,))\n",
    "trainer.push_to_hub(model_name=f\"{model_name}-finetuned-{optimizer_name}-{task}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fitetuningpretrained pytorch from scratch\n",
    "# del model\n",
    "# del trainer\n",
    "# del args\n",
    "# #del batch\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
