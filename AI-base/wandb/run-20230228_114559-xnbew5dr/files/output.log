
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.1018, 'learning_rate': 2.2748416099088026e-07, 'epoch': 0.01}
{'loss': 1.0978, 'learning_rate': 4.5496832198176053e-07, 'epoch': 0.01}
{'loss': 1.0974, 'learning_rate': 6.824524829726408e-07, 'epoch': 0.02}
{'loss': 1.0971, 'learning_rate': 9.099366439635211e-07, 'epoch': 0.02}
{'loss': 1.0938, 'learning_rate': 1.1374208049544013e-06, 'epoch': 0.03}
{'loss': 1.08, 'learning_rate': 1.3649049659452815e-06, 'epoch': 0.03}
{'loss': 1.0317, 'learning_rate': 1.592389126936162e-06, 'epoch': 0.04}
{'loss': 0.9712, 'learning_rate': 1.8198732879270421e-06, 'epoch': 0.04}
{'loss': 0.9027, 'learning_rate': 2.0473574489179223e-06, 'epoch': 0.05}
{'loss': 0.8244, 'learning_rate': 2.2748416099088027e-06, 'epoch': 0.05}
{'loss': 0.7622, 'learning_rate': 2.5023257708996827e-06, 'epoch': 0.06}
{'loss': 0.7021, 'learning_rate': 2.729809931890563e-06, 'epoch': 0.06}
{'loss': 0.6855, 'learning_rate': 2.9572940928814434e-06, 'epoch': 0.07}
{'loss': 0.6651, 'learning_rate': 3.184778253872324e-06, 'epoch': 0.07}
{'loss': 0.6552, 'learning_rate': 3.4122624148632042e-06, 'epoch': 0.08}
{'loss': 0.6708, 'learning_rate': 3.6397465758540842e-06, 'epoch': 0.08}
{'loss': 0.6308, 'learning_rate': 3.867230736844965e-06, 'epoch': 0.09}
{'loss': 0.7075, 'learning_rate': 4.094714897835845e-06, 'epoch': 0.09}
{'loss': 0.6518, 'learning_rate': 4.322199058826726e-06, 'epoch': 0.1}
{'loss': 0.693, 'learning_rate': 4.549683219817605e-06, 'epoch': 0.1}
{'loss': 0.6621, 'learning_rate': 4.777167380808486e-06, 'epoch': 0.11}
{'loss': 0.6756, 'learning_rate': 5.004651541799365e-06, 'epoch': 0.11}
{'loss': 0.6737, 'learning_rate': 5.2321357027902466e-06, 'epoch': 0.12}
{'loss': 0.6025, 'learning_rate': 5.459619863781126e-06, 'epoch': 0.12}
{'loss': 0.6594, 'learning_rate': 5.6871040247720065e-06, 'epoch': 0.13}
{'loss': 0.6603, 'learning_rate': 5.914588185762887e-06, 'epoch': 0.13}
{'loss': 0.6386, 'learning_rate': 6.142072346753767e-06, 'epoch': 0.14}
{'loss': 0.6739, 'learning_rate': 6.369556507744648e-06, 'epoch': 0.14}
{'loss': 0.6627, 'learning_rate': 6.597040668735528e-06, 'epoch': 0.15}
The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis. If premise, idx, hypothesis are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9815
  Batch size = 4
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-14727
Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/config.json
{'eval_loss': 0.6386855840682983, 'eval_accuracy': 0.8178298522669384, 'eval_runtime': 122.9571, 'eval_samples_per_second': 79.825, 'eval_steps_per_second': 19.958, 'epoch': 0.15}
Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/special_tokens_map.json
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
{'train_runtime': 3613.7852, 'train_samples_per_second': 16.3, 'train_steps_per_second': 4.075, 'train_loss': 0.7910631689705107, 'epoch': 0.15}
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-14727 (score: 0.8178298522669384).
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Several commits (2) will be pushed upstream.
The progress bars may be unreliable.
remote: Scanning LFS files of refs/heads/main for validity...
remote: LFS file scan complete.
To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli
   3873f8a..135efac  main -> main
No such comm: cf34a87a336844458296ebf4ec09cf29
cuda
{'accuracy': 0.5625}
Found cached dataset glue (/home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Exception in thread SystemMonitor:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py", line 118, in _start
    asset.start()
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py", line 166, in start
    self.metrics_monitor.start()
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py", line 168, in start
    logger.info(f"Started {self._process.name}")
AttributeError: 'NoneType' object has no attribute 'name'
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading file vocab.txt from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/vocab.txt
loading file tokenizer.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.
Sentence 2: Product and geography are what make cream skimming work.
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e918085800bdaf35.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19eaff48a7946f26.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fbc8360796e1193d.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d767475232a1637b.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-61b7eb49a8b6e726.arrow
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading weights file pytorch_model.bin from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/pytorch_model.bin
15.0 392702 0.15
Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
/home/nico/Documents/IAS2/IAssistant/AI-base/mpnet-base-finetuned-AdamW-mnli is already a clone of https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli. Make sure you pull the latest changes with `repo.git_pull()`.
The following columns in the training set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis. If premise, idx, hypothesis are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
/home/nico/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 392702
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 14727
  Number of trainable parameters = 109488771
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
