
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.0972, 'learning_rate': 1.7061283110348304e-07, 'epoch': 0.01}
{'loss': 1.0993, 'learning_rate': 3.412256622069661e-07, 'epoch': 0.01}
{'loss': 1.0991, 'learning_rate': 5.118384933104492e-07, 'epoch': 0.02}
{'loss': 1.0964, 'learning_rate': 6.824513244139321e-07, 'epoch': 0.02}
{'loss': 1.0964, 'learning_rate': 8.530641555174153e-07, 'epoch': 0.03}
{'loss': 1.0885, 'learning_rate': 1.0236769866208984e-06, 'epoch': 0.03}
{'loss': 1.0412, 'learning_rate': 1.1942898177243815e-06, 'epoch': 0.04}
{'loss': 0.9571, 'learning_rate': 1.3649026488278643e-06, 'epoch': 0.04}
{'loss': 0.8847, 'learning_rate': 1.5355154799313475e-06, 'epoch': 0.05}
{'loss': 0.82, 'learning_rate': 1.7061283110348306e-06, 'epoch': 0.05}
{'loss': 0.7683, 'learning_rate': 1.8767411421383134e-06, 'epoch': 0.06}
{'loss': 0.7126, 'learning_rate': 2.047353973241797e-06, 'epoch': 0.06}
{'loss': 0.7062, 'learning_rate': 2.2179668043452795e-06, 'epoch': 0.07}
{'loss': 0.664, 'learning_rate': 2.388579635448763e-06, 'epoch': 0.07}
{'loss': 0.647, 'learning_rate': 2.559192466552246e-06, 'epoch': 0.08}
{'loss': 0.6543, 'learning_rate': 2.7298052976557286e-06, 'epoch': 0.08}
{'loss': 0.6335, 'learning_rate': 2.900418128759212e-06, 'epoch': 0.09}
{'loss': 0.701, 'learning_rate': 3.071030959862695e-06, 'epoch': 0.09}
{'loss': 0.6602, 'learning_rate': 3.2416437909661777e-06, 'epoch': 0.1}
{'loss': 0.6874, 'learning_rate': 3.412256622069661e-06, 'epoch': 0.1}
{'loss': 0.6518, 'learning_rate': 3.582869453173144e-06, 'epoch': 0.11}
{'loss': 0.6871, 'learning_rate': 3.753482284276627e-06, 'epoch': 0.11}
{'loss': 0.6864, 'learning_rate': 3.92409511538011e-06, 'epoch': 0.12}
{'loss': 0.6193, 'learning_rate': 4.094707946483594e-06, 'epoch': 0.12}
{'loss': 0.6644, 'learning_rate': 4.265320777587076e-06, 'epoch': 0.13}
{'loss': 0.6685, 'learning_rate': 4.435933608690559e-06, 'epoch': 0.13}
{'loss': 0.636, 'learning_rate': 4.606546439794043e-06, 'epoch': 0.14}
{'loss': 0.678, 'learning_rate': 4.777159270897526e-06, 'epoch': 0.14}
{'loss': 0.6722, 'learning_rate': 4.947772102001009e-06, 'epoch': 0.15}
{'loss': 0.5993, 'learning_rate': 5.118384933104492e-06, 'epoch': 0.15}
{'loss': 0.6024, 'learning_rate': 5.288997764207974e-06, 'epoch': 0.16}
{'loss': 0.627, 'learning_rate': 5.459610595311457e-06, 'epoch': 0.16}
{'loss': 0.6319, 'learning_rate': 5.630223426414941e-06, 'epoch': 0.17}
{'loss': 0.6517, 'learning_rate': 5.800836257518424e-06, 'epoch': 0.17}
{'loss': 0.6374, 'learning_rate': 5.971449088621907e-06, 'epoch': 0.18}
{'loss': 0.6172, 'learning_rate': 6.14206191972539e-06, 'epoch': 0.18}
{'loss': 0.6305, 'learning_rate': 6.312674750828872e-06, 'epoch': 0.19}
{'loss': 0.6381, 'learning_rate': 6.483287581932355e-06, 'epoch': 0.19}
{'loss': 0.6338, 'learning_rate': 6.653900413035839e-06, 'epoch': 0.2}
The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx. If hypothesis, premise, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9815
  Batch size = 4
{'eval_loss': 0.6002181768417358, 'eval_accuracy': 0.8314824248599083, 'eval_runtime': 122.0736, 'eval_samples_per_second': 80.402, 'eval_steps_per_second': 20.103, 'epoch': 0.2}
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-19636
Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/special_tokens_map.json
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-19636 (score: 0.8314824248599083).
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
{'train_runtime': 4805.8006, 'train_samples_per_second': 16.343, 'train_steps_per_second': 4.086, 'train_loss': 0.7511148206035142, 'epoch': 0.2}
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Several commits (2) will be pushed upstream.
The progress bars may be unreliable.
remote: Scanning LFS files of refs/heads/main for validity...
remote: LFS file scan complete.
To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli
   46ef91a..47fdba8  main -> main
cuda
{'accuracy': 0.5625}
Found cached dataset glue (/home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Exception in thread SystemMonitor:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py", line 118, in _start
    asset.start()
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py", line 166, in start
    self.metrics_monitor.start()
  File "/home/nico/.local/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py", line 168, in start
    logger.info(f"Started {self._process.name}")
AttributeError: 'NoneType' object has no attribute 'name'
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading file vocab.txt from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/vocab.txt
loading file tokenizer.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.
Sentence 2: Product and geography are what make cream skimming work.
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e918085800bdaf35.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19eaff48a7946f26.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fbc8360796e1193d.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d767475232a1637b.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-61b7eb49a8b6e726.arrow
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading weights file pytorch_model.bin from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/pytorch_model.bin
20.0 392702 0.2
Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
/home/nico/Documents/IAS2/IAssistant/AI-base/mpnet-base-finetuned-AdamW-mnli is already a clone of https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli. Make sure you pull the latest changes with `repo.git_pull()`.
The following columns in the training set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx. If hypothesis, premise, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
/home/nico/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 392702
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 19636
  Number of trainable parameters = 109488771
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.0983, 'learning_rate': 1.7061283110348304e-07, 'epoch': 0.01}
{'loss': 1.0988, 'learning_rate': 3.412256622069661e-07, 'epoch': 0.01}
{'loss': 1.0985, 'learning_rate': 5.118384933104492e-07, 'epoch': 0.02}
{'loss': 1.0976, 'learning_rate': 6.824513244139321e-07, 'epoch': 0.02}
{'loss': 1.0971, 'learning_rate': 8.530641555174153e-07, 'epoch': 0.03}
{'loss': 1.0933, 'learning_rate': 1.0236769866208984e-06, 'epoch': 0.03}
{'loss': 1.0743, 'learning_rate': 1.1942898177243815e-06, 'epoch': 0.04}
{'loss': 1.0062, 'learning_rate': 1.3649026488278643e-06, 'epoch': 0.04}
{'loss': 0.9162, 'learning_rate': 1.5355154799313475e-06, 'epoch': 0.05}
{'loss': 0.837, 'learning_rate': 1.7061283110348306e-06, 'epoch': 0.05}
{'loss': 0.7676, 'learning_rate': 1.8767411421383134e-06, 'epoch': 0.06}
{'loss': 0.7102, 'learning_rate': 2.047353973241797e-06, 'epoch': 0.06}
{'loss': 0.6912, 'learning_rate': 2.2179668043452795e-06, 'epoch': 0.07}
{'loss': 0.6585, 'learning_rate': 2.388579635448763e-06, 'epoch': 0.07}
{'loss': 0.6377, 'learning_rate': 2.559192466552246e-06, 'epoch': 0.08}
{'loss': 0.656, 'learning_rate': 2.7298052976557286e-06, 'epoch': 0.08}
{'loss': 0.6315, 'learning_rate': 2.900418128759212e-06, 'epoch': 0.09}
{'loss': 0.6937, 'learning_rate': 3.071030959862695e-06, 'epoch': 0.09}
{'loss': 0.6588, 'learning_rate': 3.2416437909661777e-06, 'epoch': 0.1}
{'loss': 0.6794, 'learning_rate': 3.412256622069661e-06, 'epoch': 0.1}
{'loss': 0.6411, 'learning_rate': 3.582869453173144e-06, 'epoch': 0.11}
{'loss': 0.6773, 'learning_rate': 3.753482284276627e-06, 'epoch': 0.11}
{'loss': 0.677, 'learning_rate': 3.92409511538011e-06, 'epoch': 0.12}
{'loss': 0.6082, 'learning_rate': 4.094707946483594e-06, 'epoch': 0.12}
{'loss': 0.655, 'learning_rate': 4.265320777587076e-06, 'epoch': 0.13}
{'loss': 0.6632, 'learning_rate': 4.435933608690559e-06, 'epoch': 0.13}
{'loss': 0.6382, 'learning_rate': 4.606546439794043e-06, 'epoch': 0.14}
{'loss': 0.6726, 'learning_rate': 4.777159270897526e-06, 'epoch': 0.14}
{'loss': 0.6531, 'learning_rate': 4.947772102001009e-06, 'epoch': 0.15}
{'loss': 0.5954, 'learning_rate': 5.118384933104492e-06, 'epoch': 0.15}
{'loss': 0.5999, 'learning_rate': 5.288997764207974e-06, 'epoch': 0.16}
{'loss': 0.6388, 'learning_rate': 5.459610595311457e-06, 'epoch': 0.16}
{'loss': 0.6466, 'learning_rate': 5.630223426414941e-06, 'epoch': 0.17}
{'loss': 0.6681, 'learning_rate': 5.800836257518424e-06, 'epoch': 0.17}
{'loss': 0.6176, 'learning_rate': 5.971449088621907e-06, 'epoch': 0.18}
{'loss': 0.6271, 'learning_rate': 6.14206191972539e-06, 'epoch': 0.18}
{'loss': 0.6194, 'learning_rate': 6.312674750828872e-06, 'epoch': 0.19}
{'loss': 0.6421, 'learning_rate': 6.483287581932355e-06, 'epoch': 0.19}
{'loss': 0.6354, 'learning_rate': 6.653900413035839e-06, 'epoch': 0.2}
The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx. If hypothesis, premise, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9815
  Batch size = 4
{'eval_loss': 0.5806006789207458, 'eval_accuracy': 0.83555781966378, 'eval_runtime': 121.7257, 'eval_samples_per_second': 80.632, 'eval_steps_per_second': 20.16, 'epoch': 0.2}
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-19636
Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-19636/special_tokens_map.json
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
{'train_runtime': 4782.5407, 'train_samples_per_second': 16.422, 'train_steps_per_second': 4.106, 'train_loss': 0.7520157823156597, 'epoch': 0.2}
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-19636 (score: 0.83555781966378).
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Several commits (2) will be pushed upstream.
The progress bars may be unreliable.
remote: Scanning LFS files of refs/heads/main for validity...
remote: LFS file scan complete.
To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli
   47fdba8..83654da  main -> main
cuda
Found cached dataset glue (/home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
{'accuracy': 0.5625}
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading file vocab.txt from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/vocab.txt
loading file tokenizer.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e918085800bdaf35.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19eaff48a7946f26.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-fbc8360796e1193d.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d767475232a1637b.arrow
Loading cached processed dataset at /home/nico/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-61b7eb49a8b6e726.arrow
Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.
Sentence 2: Product and geography are what make cream skimming work.
loading configuration file config.json from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/config.json
Model config MPNetConfig {
  "_name_or_path": "microsoft/mpnet-base",
  "architectures": [
    "MPNetForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "mpnet",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.26.1",
  "vocab_size": 30527
}
loading weights file pytorch_model.bin from cache at /home/nico/.cache/huggingface/hub/models--microsoft--mpnet-base/snapshots/5b7474c98ab5f1801502f9d2348485acf4cbbe71/pytorch_model.bin
Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
20.0 392702 0.2
/home/nico/Documents/IAS2/IAssistant/AI-base/mpnet-base-finetuned-AdamW-mnli is already a clone of https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli. Make sure you pull the latest changes with `repo.git_pull()`.
The following columns in the training set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, premise, idx. If hypothesis, premise, idx are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
/home/nico/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 392702
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 19636
  Number of trainable parameters = 109488771
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
