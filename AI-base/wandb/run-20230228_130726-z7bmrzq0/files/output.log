
You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.0999, 'learning_rate': 2.2748416099088026e-07, 'epoch': 0.01}
{'loss': 1.0999, 'learning_rate': 4.5496832198176053e-07, 'epoch': 0.01}
{'loss': 1.0986, 'learning_rate': 6.824524829726408e-07, 'epoch': 0.02}
{'loss': 1.0975, 'learning_rate': 9.099366439635211e-07, 'epoch': 0.02}
{'loss': 1.0933, 'learning_rate': 1.1374208049544013e-06, 'epoch': 0.03}
{'loss': 1.0665, 'learning_rate': 1.3649049659452815e-06, 'epoch': 0.03}
{'loss': 1.023, 'learning_rate': 1.592389126936162e-06, 'epoch': 0.04}
{'loss': 0.9733, 'learning_rate': 1.8198732879270421e-06, 'epoch': 0.04}
{'loss': 0.9125, 'learning_rate': 2.0473574489179223e-06, 'epoch': 0.05}
{'loss': 0.8257, 'learning_rate': 2.2748416099088027e-06, 'epoch': 0.05}
{'loss': 0.7557, 'learning_rate': 2.5023257708996827e-06, 'epoch': 0.06}
{'loss': 0.7004, 'learning_rate': 2.729809931890563e-06, 'epoch': 0.06}
{'loss': 0.6811, 'learning_rate': 2.9572940928814434e-06, 'epoch': 0.07}
{'loss': 0.6639, 'learning_rate': 3.184778253872324e-06, 'epoch': 0.07}
{'loss': 0.656, 'learning_rate': 3.4122624148632042e-06, 'epoch': 0.08}
{'loss': 0.6839, 'learning_rate': 3.6397465758540842e-06, 'epoch': 0.08}
{'loss': 0.6297, 'learning_rate': 3.867230736844965e-06, 'epoch': 0.09}
{'loss': 0.7011, 'learning_rate': 4.094714897835845e-06, 'epoch': 0.09}
{'loss': 0.6446, 'learning_rate': 4.322199058826726e-06, 'epoch': 0.1}
{'loss': 0.6698, 'learning_rate': 4.549683219817605e-06, 'epoch': 0.1}
{'loss': 0.6525, 'learning_rate': 4.777167380808486e-06, 'epoch': 0.11}
{'loss': 0.6896, 'learning_rate': 5.004651541799365e-06, 'epoch': 0.11}
{'loss': 0.6806, 'learning_rate': 5.2321357027902466e-06, 'epoch': 0.12}
{'loss': 0.614, 'learning_rate': 5.459619863781126e-06, 'epoch': 0.12}
{'loss': 0.644, 'learning_rate': 5.6871040247720065e-06, 'epoch': 0.13}
{'loss': 0.6387, 'learning_rate': 5.914588185762887e-06, 'epoch': 0.13}
{'loss': 0.6502, 'learning_rate': 6.142072346753767e-06, 'epoch': 0.14}
{'loss': 0.6914, 'learning_rate': 6.369556507744648e-06, 'epoch': 0.14}
{'loss': 0.6595, 'learning_rate': 6.597040668735528e-06, 'epoch': 0.15}
The following columns in the evaluation set don't have a corresponding argument in `MPNetForSequenceClassification.forward` and have been ignored: hypothesis, idx, premise. If hypothesis, idx, premise are not expected by `MPNetForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 9815
  Batch size = 4
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli/checkpoint-14727
Configuration saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/config.json
{'eval_loss': 0.6111095547676086, 'eval_accuracy': 0.8210901681100357, 'eval_runtime': 126.3267, 'eval_samples_per_second': 77.695, 'eval_steps_per_second': 19.426, 'epoch': 0.15}
Model weights saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/checkpoint-14727/special_tokens_map.json
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
wandb: Network error (ConnectionError), entering retry loop.
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from mpnet-base-finetuned-AdamW-mnli/checkpoint-14727 (score: 0.8210901681100357).
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Saving model checkpoint to mpnet-base-finetuned-AdamW-mnli
Configuration saved in mpnet-base-finetuned-AdamW-mnli/config.json
Model weights saved in mpnet-base-finetuned-AdamW-mnli/pytorch_model.bin
tokenizer config file saved in mpnet-base-finetuned-AdamW-mnli/tokenizer_config.json
Special tokens file saved in mpnet-base-finetuned-AdamW-mnli/special_tokens_map.json
Several commits (2) will be pushed upstream.
The progress bars may be unreliable.
{'train_runtime': 3708.396, 'train_samples_per_second': 15.884, 'train_steps_per_second': 3.971, 'train_loss': 0.7899580418217682, 'epoch': 0.15}
remote: Scanning LFS files of refs/heads/main for validity...
remote: LFS file scan complete.
To https://huggingface.co/NicolasLe/mpnet-base-finetuned-AdamW-mnli
